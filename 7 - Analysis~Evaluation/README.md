# Analysis/Evaluation
#### _"Continous Improvement is Better than delayed perfection"_

## [Evaluation Metrics](https://github.com/pauloreis-ds/Machine-Learning-ROADMAP/blob/master/7%20-%20Analysis~Evaluation/Metrics.md)
**You need to master these, so there's no way you would let a bad model be deployed and make the company lose a lot of money because of it.**

    Classification:
        Confusion Matrix
        Accuracy
        Precision
        Recall
        F1
        Mean Average Precision (object detection)
    
    Regression:
        MSE (Mean Squared Error)
        MAE (Mean Absolute Error)
        R^2 (r-squared)
        
    Task-Based Metric:
        Create one based on your specific (domain knowledge) problem.

## Feature Importance
- Which feature contributed most to the model? Should some be removed?

## Training/Inference Time/Cost
- How long does a model take to train? Is this feasible?
- How long does inference take? Is it suitable for production?

## Least Confident Examples
- What does the model get wrong? (usually instances you don't have many examples of)

## Bias/Variance Trade-off
- High bias results in underfitting and lack of generalization to new samples.
- High Variance results in ovefitting due to the model finding patterns which is actually random noise.

[WTF is the Bias-Variance Tradeoff? (Infographic)](https://elitedatascience.com/bias-variance-tradeoff)




[<img align="right" width="60" height="60" src="https://github.com/pauloreis-ds/Paulo-Reis-Data-Science/blob/master/Paulo%20Reis/Pauloreis01.png">](https://github.com/pauloreis-ds)

---
